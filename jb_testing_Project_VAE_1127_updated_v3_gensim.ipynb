{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ee899a-e724-4b11-90b8-12cd4f2ac1cc",
   "metadata": {},
   "source": [
    "# VAE for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8910863-3172-49b3-84b6-9f241791fb0e",
   "metadata": {},
   "source": [
    "## Key Concepts:\n",
    "1. `Encoder/Decoder`: The `VAE` architecture has an encoder that maps documents to a latent space and a decoder that attempts to reconstruct the input from the latent space.\n",
    "2. `Latent Space`: The latent variables in the `VAE` can represent topics. By clustering or examining this space, topics can be discovered.\n",
    "3. `Loss Function`: Combines reconstruction loss (how well the input is reconstructed) and KL divergence (encouraging the latent space to follow a normal distribution).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "359119be-facd-48e4-9e1d-4084e56d14b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2aca1-cb5d-4f7f-92c4-ffe64f5abd28",
   "metadata": {},
   "source": [
    "## Sample dataset: List of documents\n",
    "\n",
    "tf.clip_by_global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4531aa87-24f5-4660-9bb8-45969c4c2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = pd.read_pickle('scied_words_bigrams_V5.pkl')\n",
    "documents_full = data_words_bigrams #taking first group of documents in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d465cde-f7af-4e94-951b-5b96f7ac7f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5577, 1500)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# Load the data\n",
    "#data_words_bigrams = pd.read_pickle('scied_words_bigrams_V5.pkl')\n",
    " \n",
    "# Take the first 10 documents\n",
    "documents = data_words_bigrams  # Represents all words in the first 10 documents\n",
    " \n",
    "# Gensim filtering\n",
    "no_below = 15  # Keep tokens which are contained in at least 15 documents\n",
    "no_above = 0.5  # Remove tokens that are contained in more than 50% of the documents\n",
    "id2word = gensim.corpora.Dictionary(documents)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)\n",
    " \n",
    "# Convert documents to Bag-of-Words representation\n",
    "bow_corpus = [id2word.doc2bow(doc) for doc in documents]\n",
    " \n",
    "# Prepare documents for CountVectorizer\n",
    "documents = [\" \".join(doc) for doc in documents]\n",
    " \n",
    "# Generate Bag-of-Words matrix\n",
    "vectorizer = CountVectorizer(max_features=1500)\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    " \n",
    "# Normalize the BoW matrix using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_normalized = scaler.fit_transform(X)\n",
    " \n",
    "# Check the shape of the normalized data\n",
    "print(X_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "284ba5f0-204a-490a-8fd1-f0292b509add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05555556, 0.45945946, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.01587302, 0.02702703, 0.00869565, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01587302, 0.02702703, 0.        , ..., 0.        , 0.00943396,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.01639344]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62bfa5ee-ef7e-4e29-9460-d48736578d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_full = [\" \".join(doc) for doc in documents_full]\n",
    "documents = documents_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c7065c-8507-4ee7-9ddb-ec7e02a7458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def count_word_frequencies_to_dataframe(documents):\n",
    "    \"\"\"\n",
    "    Count the frequency of each word across all documents and return a DataFrame.\n",
    "\n",
    "    :param documents: List of strings, where each string is a document.\n",
    "    :return: A Pandas DataFrame with columns 'word' and 'count', sorted by 'count' in descending order.\n",
    "    \"\"\"\n",
    "    # Flatten all documents into one list of words\n",
    "    all_words = []\n",
    "    for doc in documents:\n",
    "        all_words.extend(doc.split())  # Split each document into words and add to the list\n",
    "\n",
    "    # Use Counter to count word frequencies\n",
    "    word_frequencies = Counter(all_words)\n",
    "\n",
    "    # Convert the Counter to a DataFrame\n",
    "    df = pd.DataFrame(word_frequencies.items(), columns=['word', 'count'])\n",
    "\n",
    "    # Sort the DataFrame by 'count' in descending order\n",
    "    df = df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1de3d2c-2b8b-46bc-951f-e91278a43a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_word_frequencies_to_dataframe(documents).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7423b460-76d5-4821-8aed-155b6fc24afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = {\"science\", \"student\", \"teacher\", \"study\", \"teach\", \"learn\", \"education\", \"school\", \"group\", \"course\"}\n",
    "\n",
    "def remove_common_words(documents, common_words):\n",
    "    \"\"\"\n",
    "    Remove specified common words from a list of documents.\n",
    "\n",
    "    :param documents: List of strings, where each string is a document.\n",
    "    :param common_words: Set of words to remove.\n",
    "    :return: List of documents with the common words removed.\n",
    "    \"\"\"\n",
    "    cleaned_documents = []\n",
    "    for doc in documents:\n",
    "        # Split the document into words, remove common words, and join it back\n",
    "        filtered_words = [word for word in doc.split() if word.lower() not in common_words]\n",
    "        cleaned_documents.append(' '.join(filtered_words))\n",
    "    return cleaned_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27653736-2a97-438a-b090-9109d829df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = remove_common_words(documents, common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c8ee3-cf86-4fbd-beec-24e0701a1afa",
   "metadata": {},
   "source": [
    "## Preprocessing text data using CountVectorizer\n",
    "* We use `CountVectorizer` from `scikit-learn` to transform the documents into a `Bag-of-Words` representation.  \n",
    "* This means that each document is converted into a vector where each element represents the frequency of a word in the document.  \n",
    "* `max_features=1000` limits the number of words (or features) to 1,000, though this is more relevant for larger corpora.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dd68ec9-7a82-415d-b4fc-f41619b93003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(max_features=1500)\n",
    "#X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2741c379-8421-45eb-be09-0b5ebadf0432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5577, 1500)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c298e43-9c37-40d9-9adf-f6381e4ca268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X\n",
    "X = X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26aa267-2005-4a71-af9b-165085e57ba7",
   "metadata": {},
   "source": [
    "## Define Variational Autoencoder (VAE) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8632608f-b05f-48df-808e-300ccec84291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Architecture\n",
    "#1. `Encoder/Decoder`: The `VAE` architecture has an encoder that maps documents to a latent space and a decoder that attempts to reconstruct the input from the latent space.\n",
    "#2. `Latent Space`: The latent variables in the `VAE` can represent topics. By clustering or examining this space, topics can be discovered.\n",
    "#3. `Loss Function`: Combines reconstruction loss (how well the input is reconstructed) and KL divergence (encouraging the latent space to follow a normal distribution).  \n",
    "\n",
    "#`encode()`: The encoder splits the output into mean and logvar (log variance).\n",
    "#`reparameterize()`: Instead of directly using the mean and log variance, this function samples from the Gaussian distribution defined by the mean and variance. The reason for this \"reparameterization trick\" is to allow backpropagation through the stochastic sampling process.\n",
    "#`sample()`: Once trained, can call this functiion to use the decoder to generate new samples (e.g., new document representations) by sampling from the latent space.\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, original_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder with careful initialization\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(original_dim,)),\n",
    "            layers.Dense(512, \n",
    "                activation='relu', \n",
    "                kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer='zeros'),\n",
    "            layers.Dense(256, \n",
    "                activation='relu', \n",
    "                kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer='zeros'),\n",
    "            layers.Dense(latent_dim * 2,  \n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                bias_initializer='zeros')\n",
    "        ])\n",
    "        \n",
    "        # Decoder with careful initialization\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            layers.Dense(256, \n",
    "                activation='relu', \n",
    "                kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer='zeros'),\n",
    "            layers.Dense(512, \n",
    "                activation='relu', \n",
    "                kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer='zeros'),\n",
    "            layers.Dense(original_dim, \n",
    "                activation='sigmoid',  # Ensure output is between 0 and 1\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                bias_initializer='zeros')\n",
    "        ])\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # Add numerical stability checks\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        \n",
    "        # Split output into mean and log variance\n",
    "        z = self.encoder(x)\n",
    "        mean, logvar = tf.split(z, num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        # Clip log variance to prevent extreme values\n",
    "        logvar = tf.clip_by_value(logvar, -10, 10)\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Stable reparameterization trick\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        eps = tf.random.normal(tf.shape(mean))\n",
    "        return mean + std * eps\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=True):\n",
    "        # Decode and optionally apply sigmoid\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            return tf.sigmoid(logits)\n",
    "        return logits\n",
    "    \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.decoder.input_shape[1]))\n",
    "        return self.decode(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43468455-2775-4e5e-b977-bdf3fca8ed13",
   "metadata": {},
   "source": [
    "## Compute loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "364f1ada-1307-43cc-a6ee-867d62090be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The loss function in a VAE combines two parts:\n",
    "#1. `Reconstruction Loss`: Measures how well the model can reconstruct the input from the latent space.\n",
    "#2. `KL Divergence`: Encourages the distribution of latent variables to be close to a normal distribution (helps with regularization).\n",
    "\n",
    "#`Reconstruction loss`: This is the binary cross-entropy between the original input x and its reconstruction x_logit. It is summed across all dimensions (words) of the document.  \n",
    "\n",
    "#`KL divergence`: This term encourages the latent variable distribution to stay close to a unit normal distribution (with mean 0 and variance 1). This helps regularize the model and ensure that the latent space is structured.  \n",
    "\n",
    "#The final loss is the sum of the reconstruction loss and KL divergence, and we minimize this during training.\n",
    "\n",
    "def kl_divergence_loss(mean, logvar):\n",
    "    # Standard KL divergence loss between the learned distribution and standard normal\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    return kl_loss\n",
    "\n",
    "def compute_loss(model, x, beta=1.0):\n",
    "    x = tf.clip_by_value(tf.cast(x, tf.float32), 1e-7, 1 - 1e-7)\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_recon = model.decode(z)\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(\n",
    "        x_recon - x * tf.math.log(x_recon + 1e-7), axis=-1\n",
    "    ))\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    \n",
    "    # Total loss with adjustable beta\n",
    "    total_loss = reconstruction_loss + beta * kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226ef3e-b86e-4f88-9092-e10ae7ccdd9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4bc6bac-f6d6-440b-b61d-de093dcb7839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JonBr\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training loop modifications\n",
    "latent_dim = 15 #Latent dimensions to use\n",
    "original_dim = X.shape[1] #Original shape of the input data\n",
    "vae = VAE(original_dim, latent_dim) #Init VAE model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  #Setting the learning rate\n",
    "epochs = 100  #Number of training epochs\n",
    "batch_size = 32  #Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba41e8-5762-4fd7-9322-a0efceafec09",
   "metadata": {},
   "source": [
    "## Training the VAE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbeef94-80ef-42c1-b938-1151b325e660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 773.776985606928\n",
      "Epoch 1, Loss: 769.5078837076823\n",
      "Epoch 2, Loss: 769.5078237248563\n",
      "Epoch 3, Loss: 769.5078132015535\n",
      "Epoch 4, Loss: 769.5077907518408\n",
      "Epoch 5, Loss: 769.5077714591191\n",
      "Epoch 6, Loss: 769.5077725114494\n",
      "Epoch 7, Loss: 769.5077507632902\n",
      "Epoch 8, Loss: 769.5077448000853\n",
      "Epoch 9, Loss: 769.5077412923177\n",
      "Epoch 10, Loss: 769.5077367322199\n",
      "Epoch 11, Loss: 769.5077300674614\n"
     ]
    }
   ],
   "source": [
    "#* `Optimizer`: We use the Adam optimizer to minimize the loss function.\n",
    "\n",
    "#* `Training Loop`: We loop over the data in batches (batch size 2 in this case) for a number of epochs. Inside the loop:\n",
    "\n",
    "#A mini-batch of documents (x_batch) is fed into the VAE.\n",
    "#The loss is computed using the compute_loss function.\n",
    "#The gradients are calculated using TensorFlowâ€™s GradientTape, and the model is updated accordingly.\n",
    "#After each epoch, the current loss is printed out to track progress.\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X), 32):\n",
    "        x_batch = X[i:i+32]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(vae, x_batch)\n",
    "        \n",
    "        gradients = tape.gradient(loss, vae.trainable_variables)\n",
    "        gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n",
    "        optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {epoch_loss / (len(X) // 32)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25ecfa-788e-45c9-9dcb-d442f4ade797",
   "metadata": {},
   "source": [
    "## Interpreting the Latent Space\n",
    "* The latent space can be used to represent topics  \n",
    "* Can cluster the latent vectors to interpret the topics\n",
    "* z represents the latent space, and clustering these latent representations will give groups of documents that share similar latent features, which can be interpreted as topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19f93b5b-f7d9-441a-bca7-44df229149a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean, logvar = vae.encode(X)\n",
    "z = vae.reparameterize(mean, logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1213d-280e-441e-bc3d-9d7a9649b5b4",
   "metadata": {},
   "source": [
    "# Steps to Visualize Documents by Topic:\n",
    "1. Obtain Latent Vectors: After training the VAE, extract the latent vectors (z) for each document.\n",
    "2. Assign Topics: Assign topics to documents by clustering the latent vectors (using KMeans or another clustering algorithm).\n",
    "3. Dimensionality Reduction: Use t-SNE or PCA to reduce the dimensionality of the latent vectors to 2D or 3D for visualization.\n",
    "4. Plot: Use Matplotlib to plot the projections and color them by their assigned topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ffcd4-8249-4552-a581-6e95ac76b86a",
   "metadata": {},
   "source": [
    "The visualization shows a scatter plot of documents mapped to a 2D latent space, where the colors represent different topic clusters as identified by the KMeans clustering algorithm. However, the plot alone doesn't tell us what the actual topics are in terms of words or themes.\n",
    "\n",
    "To determine the topics, you need to inspect the original document vectors and see which words (or features) are most associated with each topic. There are a couple of ways to extract and interpret the topics:\n",
    "\n",
    "1. Get the Top Words per Topic:\n",
    "After clustering the documents, you can examine which words or features are most influential in each topic cluster by looking at the original word distributions for the documents assigned to each topic.\n",
    "\n",
    "Approach:\n",
    "Assign the topics to each document and, for each topic, sum the word counts across the documents assigned to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0686b65d-ac65-4964-81a3-d74aff00d300",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=9 should be >= n_clusters=23.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m23\u001b[39m  \u001b[38;5;66;03m# You can adjust this depending on how many topics you expect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mnum_topics, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m topics \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(z)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1070\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1473\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1465\u001b[0m     X,\n\u001b[0;32m   1466\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1470\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1471\u001b[0m )\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1475\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1476\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1414\u001b[0m, in \u001b[0;36mKMeans._check_params_vs_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1414\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X, default_n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melkan\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:878\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[1;34m(self, X, default_n_init)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, default_n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[1;32m--> 878\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    879\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[1;31mValueError\u001b[0m: n_samples=9 should be >= n_clusters=23."
     ]
    }
   ],
   "source": [
    "# `z` is the latent space representation of the documents\n",
    "\n",
    "# Assign topics using KMeans clustering\n",
    "num_topics = 23  # You can adjust this depending on how many topics you expect\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "topics = kmeans.fit_predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471e6db-aeed-4f40-af65-ce4c40839797",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `topics` are the cluster assignments for each document\n",
    "# and `X` is the term-document matrix from CountVectorizer\n",
    "\n",
    "# Get feature names (words) from the CountVectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Add the topic assignments to the DataFrame\n",
    "df['topic'] = topics\n",
    "\n",
    "# Get top words for each topic by summing word counts for each topic\n",
    "for topic in range(num_topics):\n",
    "    print(f\"Topic {topic}:\")\n",
    "    # Get all documents assigned to this topic\n",
    "    topic_docs = df[df['topic'] == topic].drop(columns='topic')\n",
    "    \n",
    "    # Sum the word counts for this topic\n",
    "    topic_word_sums = topic_docs.sum(axis=0).sort_values(ascending=False)\n",
    "    \n",
    "    # Display the top 10 words for the topic\n",
    "    print(topic_word_sums.head(10))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec2a3a-e187-4a3b-ba53-deb89db36376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de302001-80e0-4ac6-965a-6020e983dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If `latent_dim` is 2 (for easier visualization in 2D), we skip t-SNE.\n",
    "# If latent_dim > 2, you can uncomment t-SNE for dimensionality reduction.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if z.shape[1] == 2:\n",
    "    plt.scatter(z[:, 0], z[:, 1], c=topics, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(label='Topic')\n",
    "    plt.title('Latent Space Clustering')\n",
    "    plt.show()\n",
    "else:\n",
    "    from sklearn.manifold import TSNE\n",
    "    z_2d = TSNE(n_components=2, random_state=42).fit_transform(z)\n",
    "    plt.scatter(z_2d[:, 0], z_2d[:, 1], c=topics, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(label='Topic')\n",
    "    plt.title('Latent Space Clustering (t-SNE)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d09b8-f0f8-4024-88ad-abdb0cba5406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
